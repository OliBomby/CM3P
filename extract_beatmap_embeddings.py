import logging
import sys
from pathlib import Path

import argparse
import pandas as pd
import torch
from tqdm import tqdm
from transformers.trainer_utils import set_seed

from cm3p.modeling_cm3p import CM3PModel
from cm3p.processing_cm3p import CM3PProcessor
from utils.mmrs_dataset import MmrsDataset, worker_init_fn
from utils.data_utils import filter_mmrs_metadata, load_mmrs_metadata
from config import DataSetConfig
from typing import Any

logger = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Extract CM3P beatmap embeddings to a parquet file.")
    # Model repo/name
    parser.add_argument(
        "--pretrained-model-name-or-path",
        type=str,
        default="OliBomby/CM3P",
        help="Hugging Face model repo or local path for from_pretrained.",
    )
    # Dataset paths (no split between train/test)
    parser.add_argument(
        "--dataset-paths",
        type=str,
        nargs="+",
        required=True,
        help="One or more dataset root directories. Produce these with Mapperator.",
    )
    # Index range filters
    parser.add_argument("--start", type=int, default=None, help="Dataset start index (beatmap set offset).")
    parser.add_argument("--end", type=int, default=None, help="Dataset end index (exclusive).")
    # Gamemodes
    parser.add_argument(
        "--gamemodes",
        type=int,
        nargs="+",
        default=[ 0, 1, 2, 3 ],
        help="List of gamemodes to include (e.g., 0, 1, 2, 3).",
    )
    # Year/difficulty filters
    parser.add_argument("--min-year", type=int, default=None, help="Minimum beatmap year.")
    parser.add_argument("--max-year", type=int, default=None, help="Maximum beatmap year.")
    parser.add_argument("--min-difficulty", type=float, default=None, help="Minimum difficulty rating.")
    parser.add_argument("--max-difficulty", type=float, default=None, help="Maximum difficulty rating.")
    # Loader args
    parser.add_argument("--batch-size", type=int, default=4, help="Batch size for processing features.")
    parser.add_argument("--dataloader-num-workers", type=int, default=4, help="Number of dataloader workers.")
    # Optional merge with existing embeddings
    parser.add_argument(
        "--merge-with",
        type=str,
        default=None,
        help="Path to an existing embeddings parquet generated by this script. The output will contain the union of rows, with newly generated rows replacing existing ones on matching 'Id'.",
    )
    # Misc
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility.")
    parser.add_argument(
        "--output",
        type=str,
        default="beatmap_embeddings.parquet",
        help="Output parquet file path.",
    )
    return parser.parse_args()


def build_minimal_dataset_config(ns: argparse.Namespace) -> DataSetConfig:
    # Create a minimal config that disables augmentation and ensures source metadata is included
    cfg = DataSetConfig(
        # Paths â€“ use train paths since MmrsDataset expects train_dataset_paths for non-test
        train_dataset_paths=[str(p) for p in ns.dataset_paths],
        test_dataset_paths=[str(p) for p in ns.dataset_paths],
        # Index range
        train_dataset_start=ns.start,
        train_dataset_end=ns.end,
        test_dataset_start=None,
        test_dataset_end=None,
        # Filters
        gamemodes=ns.gamemodes,
        min_year=ns.min_year,
        max_year=ns.max_year,
        min_difficulty=ns.min_difficulty,
        max_difficulty=ns.max_difficulty,
        # Behavior
        cycle_length=1,
        drop_last=False,
        include_audio=True,
        include_beatmap=True,
        include_metadata=False,
        include_source_metadata=True,
        sampling_rate=16000,
        # Labels disabled for pure embedding extraction
        labels="none",
        # Augmentations disabled/minimized
        dt_augment_prob=0.0,
        beatmap_mismatch_prob=0.0,
        metadata_dropout_prob=0.0,
        train_metadata_variations=1,
        test_metadata_variations=1,
        # Masked LM related (unused)
        masked_lm_prob=0.0,
        masked_lm_split=[0.8, 0.9, 1.0],
        dt_augment_range=[0.9, 1.1],
        dt_augment_sqrt=False,
    )
    return cfg


def main():
    ns = parse_args()

    # Early validation: if --merge-with is provided, require an existing parquet file
    if getattr(ns, "merge_with", None):
        existing_path = Path(ns.merge_with)
        if not existing_path.exists():
            print(f"Error: --merge-with path does not exist: {existing_path}")
            sys.exit(1)
        if existing_path.suffix.lower() != ".parquet":
            print(f"Error: --merge-with must point to a .parquet file: {existing_path}")
            sys.exit(1)

    set_seed(ns.seed)

    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger.setLevel(logging.INFO)

    # Initialize processor and model from_pretrained similar to README
    repo_id = ns.pretrained_model_name_or_path
    device = "cuda" if torch.cuda.is_available() else "cpu"

    logger.info(f"Loading processor from: {repo_id}")
    processor = CM3PProcessor.from_pretrained(repo_id)

    logger.info(f"Loading model from: {repo_id}")
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    model: CM3PModel = CM3PModel.from_pretrained(
        repo_id,
        device_map=device,
        dtype=dtype,
        trust_remote_code=True,
        revision="main",
    )
    model.eval()

    # Minimal dataset config (no augmentation)
    dataset_cfg = build_minimal_dataset_config(ns)

    # Load dataset (non-test for ordering). Disabling drop_last.
    dataset = MmrsDataset(
        dataset_cfg,
        processor=processor,
        test=False,
    )

    # Construct DataLoader
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=ns.batch_size,
        num_workers=ns.dataloader_num_workers,
        timeout=600 if ns.dataloader_num_workers > 0 else 0,
        worker_init_fn=worker_init_fn,
        drop_last=False,
        in_order=True,
    )

    # Load metadata and apply filters early to estimate batches and to set dynamic maps for processor if needed
    metadata = dataset.get_filtered_metadata()

    # Estimate number of batches: sum of lengths / stride / batch size
    # Use default processor kwargs if available; fall back to 16s stride
    window_stride_sec = 16
    try:
        default_kwargs = getattr(processor, "default_kwargs", {})
        beatmap_kwargs = default_kwargs.get("beatmap_kwargs", {})
        window_stride_sec = int(beatmap_kwargs.get("window_stride_sec", window_stride_sec))
    except Exception:
        pass
    est_batches = int(metadata["TotalLength"].sum() // window_stride_sec // ns.batch_size + 1)

    # Accumulators for embeddings per beatmap
    embed_accumulator: dict[int, dict[str, Any]] = {}
    # Structure: beatmap_id -> { 'sum': np.ndarray, 'count': int }

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Extracting beatmap embeddings", smoothing=0.01, total=est_batches):
            # Some batches may be empty if dataset yields nothing
            if len(batch.get("input_ids", [])) == 0:
                continue

            # Prepare inputs for model; the Processor returns dict-compatible tensors
            inputs = {
                "input_ids": batch["input_ids"].to(device),
                "attention_mask": batch["attention_mask"].to(device),
            }
            input_features = batch.get("input_features", None)
            if input_features is not None:
                inputs["input_features"] = input_features.to(device)

            outputs = model(**inputs, return_loss=False)
            # Normalized beatmap embeddings (forward applies projection + normalization)
            embeds = outputs.beatmap_embeds.detach().cpu().numpy()

            beatmap_ids = batch.get("beatmap_id", None)
            if beatmap_ids is None:
                continue
            # beatmap_ids may be tensor -> convert to list
            if torch.is_tensor(beatmap_ids):
                beatmap_ids = beatmap_ids.tolist()

            for i, bid in enumerate(beatmap_ids):
                if bid is None:
                    continue
                if bid not in embed_accumulator:
                    embed_accumulator[bid] = {
                        'sum': embeds[i].copy(),
                        'count': 1,
                    }
                else:
                    embed_accumulator[bid]['sum'] += embeds[i]
                    embed_accumulator[bid]['count'] += 1

    # Build DataFrame of mean embeddings keyed by beatmap_id
    rows = []
    for bid, info in embed_accumulator.items():
        mean_vec = info['sum'] / info['count']
        # Optional: re-normalize the averaged embedding to unit length for consistency
        norm = (mean_vec ** 2).sum() ** 0.5
        if norm > 0:
            mean_vec = mean_vec / norm
        rows.append({
            'beatmap_id': int(bid),
            'embedding': mean_vec.tolist(),
        })

    embeddings_df = pd.DataFrame(rows)

    # Merge embeddings with full metadata (which has MultiIndex [BeatmapSetId, Id])
    meta_df = metadata.reset_index()  # brings BeatmapSetId and Id into columns
    # Ensure the Id column is int for a robust merge
    if meta_df['Id'].dtype != 'int64' and meta_df['Id'].dtype != 'int32':
        meta_df['Id'] = meta_df['Id'].astype(int)
    merged_df = embeddings_df.merge(meta_df, left_on='beatmap_id', right_on='Id', how='left')

    # Reorder columns to put embedding at the end, keeping all original metadata columns
    cols = ['Artist', 'ArtistUnicode', 'Creator', 'FavouriteCount', 'BeatmapSetId', 'Nsfw', 'Offset', 'BeatmapSetPlayCount', 'Source', 'BeatmapSetStatus', 'Spotlight', 'Title', 'TitleUnicode', 'BeatmapSetUserId', 'Video', 'Description', 'GenreId', 'GenreName', 'LanguageId', 'LanguageName', 'PackTags', 'Ratings', 'DownloadDisabled', 'BeatmapSetBpm', 'CanBeHyped', 'DiscussionLocked', 'BeatmapSetIsScoreable', 'BeatmapSetLastUpdated', 'BeatmapSetRanked', 'RankedDate', 'Storyboard', 'SubmittedDate', 'Tags', 'DifficultyRating', 'Id', 'Mode', 'Status', 'TotalLength', 'UserId', 'Version', 'Checksum', 'MaxCombo', 'Accuracy', 'Ar', 'Bpm', 'CountCircles', 'CountSliders', 'CountSpinners', 'Cs', 'Drain', 'HitLength', 'IsScoreable', 'LastUpdated', 'ModeInt', 'PassCount', 'PlayCount', 'Ranked', 'Owners', 'TopTagIds', 'TopTagCounts', 'StarRating', 'OmdbTags', 'AudioFile', 'BeatmapSetFolder', 'BeatmapFile', 'embedding']
    merged_df = merged_df[cols]

    # If requested, merge into an existing embeddings parquet, preferring newly generated rows on Id collisions
    final_df = merged_df
    if getattr(ns, "merge_with", None):
        existing_path = Path(ns.merge_with)
        logger.info(f"Merging with existing embeddings at: {existing_path}")
        try:
            existing_df = pd.read_parquet(existing_path)
            # Ensure Id is int for both
            if existing_df['Id'].dtype not in ('int64', 'int32'):
                existing_df['Id'] = existing_df['Id'].astype(int)
            # Align columns to the new schema: add missing columns and drop extras
            for col in merged_df.columns:
                if col not in existing_df.columns:
                    existing_df[col] = pd.NA
            existing_df = existing_df[merged_df.columns]
            # Concat existing first, then new, and drop duplicates on Id keeping the last (new)
            final_df = pd.concat([existing_df, merged_df], ignore_index=True)
            final_df = final_df.drop_duplicates(subset=['Id'], keep='last').reset_index(drop=True)
            logger.info(f"Merged rows: existing={len(existing_df)}, new={len(merged_df)}, result={len(final_df)}")
        except Exception as e:
            logger.warning(f"Failed to merge with existing embeddings file '{existing_path}': {e}")
            final_df = merged_df

    output_path = Path(ns.output)
    final_df.to_parquet(output_path, index=False)
    logger.info(f"Final DataFrame has {final_df.shape[0]} rows.")
    logger.info(f"Final DataFrame has {final_df.shape[1]} columns.")
    logger.info(f"Columns: {final_df.columns.tolist()}")
    logger.info(f"Saved {len(final_df)} beatmap embeddings to {output_path.resolve()}")


if __name__ == "__main__":
    main()
